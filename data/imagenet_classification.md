# ImageNet Classification with Deep Convolutional Neural Networks

We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.

## Architecture

AlexNet consists of eight learned layers: five convolutional layers and three fully-connected layers. The first convolutional layer filters the 224x224x3 input image with 96 kernels of size 11x11x3 with a stride of 4 pixels. The second convolutional layer takes the output of the first layer and filters it with 256 kernels of size 5x5x48. The third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers, using 384, 384, and 256 kernels respectively. The fully-connected layers have 4096 neurons each. The network uses ReLU (Rectified Linear Unit) as its activation function, which trains several times faster than the traditional tanh function. The network was split across two NVIDIA GTX 580 GPUs with 3GB of memory each, with each GPU handling roughly half the kernels.

## Key Technical Contributions

The paper introduced several techniques that became standard in deep learning. Dropout regularization randomly sets neuron outputs to zero with probability 0.5 during training, forcing the network to learn more robust features that do not rely on the presence of specific other neurons. Local response normalization was applied after the ReLU activation in certain layers, implementing a form of lateral inhibition inspired by biological neurons. Data augmentation was used extensively: random 224x224 crops were extracted from the 256x256 images, horizontal reflections were applied, and RGB pixel intensities were altered using PCA on the training set. These augmentation strategies increased the effective training set by a factor of 2048 and significantly reduced overfitting.

## Training and Impact

The network was trained using stochastic gradient descent with a batch size of 128, momentum of 0.9, and weight decay of 0.0005. Training took five to six days on two GPUs. The initial learning rate was 0.01 and was divided by 10 when the validation error stopped improving. AlexNet's victory in ILSVRC-2012 with a top-5 error of 15.3%—almost halving the second-place error of 26.2%—is widely considered the moment that ignited the deep learning revolution in computer vision. The paper demonstrated that depth, GPU training, and techniques like dropout and data augmentation could achieve unprecedented accuracy on large-scale image recognition.

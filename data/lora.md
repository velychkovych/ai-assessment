# LoRA: Low-Rank Adaptation of Large Language Models

An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example—deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.

## Method

LoRA is motivated by the observation that pre-trained language models have a low "intrinsic dimension"—they can still learn effectively even when randomly projected to a smaller subspace. This suggests that weight updates during fine-tuning also have a low intrinsic rank. For a pre-trained weight matrix W0 ∈ R^(d×k), LoRA constrains the update by representing it with a low-rank decomposition: W0 + ΔW = W0 + BA, where B ∈ R^(d×r) and A ∈ R^(r×k), with the rank r ≪ min(d, k). During training, W0 is frozen and does not receive gradient updates, while A and B are trainable. Matrix A is initialized with a random Gaussian and B is initialized to zero, so ΔW = BA is zero at the start of training. The update is scaled by α/r, where α is a constant. At inference time, the product BA can be merged into W0, so there is no additional latency compared to the original model.

## Results and Comparisons

On GPT-3 175B, LoRA matches or exceeds full fine-tuning performance while training only 4.7 million parameters compared to 175 billion—a reduction of over 10,000 times. The GPU memory requirement drops by 3 times because there is no need to store optimizer states for the frozen parameters. LoRA outperforms several other parameter-efficient methods including adapter layers, prefix tuning, and fine-tuning only the bias terms. Unlike adapter-based methods that add sequential computation and introduce inference latency, LoRA's weight matrices can be merged at deployment, maintaining the original model's inference speed. Experiments across RoBERTa, DeBERTa, GPT-2, and GPT-3 show consistent results: LoRA achieves comparable or better accuracy with far fewer trainable parameters.

## Rank Analysis and Practical Impact

The paper provides an empirical analysis of rank-deficiency, finding that even very low ranks (r = 1 to 4) often suffice for adaptation, indicating that the weight update matrices have very low intrinsic rank. Increasing rank beyond a small threshold yields diminishing returns. This finding has important practical implications: task-specific LoRA adapters are extremely small (often just a few megabytes) and can be swapped at serving time, enabling a single base model to serve many tasks by loading different lightweight adapters. LoRA has since become the dominant method for efficient fine-tuning and is widely adopted in frameworks like Hugging Face PEFT, making it practical to customize large language models on consumer-grade hardware.
